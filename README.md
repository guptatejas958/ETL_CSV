
# ETL_CSV: Chunked CSV ETL Pipeline with SQLite Storage

## Project Overview

`ETL_CSV` is a robust and modular ETL (Extract-Transform-Load) pipeline designed for processing **very large CSV files** (up to several GBs), transforming them in **parallel**, and storing the final cleaned data into a **SQLite database**.

This project is optimized to:
- Handle CSVs larger than system memory using **chunked processing**
- Transform and clean data
- Merge results
- Insert clean records into a local database
- Log errors and status
- Run in **parallel using ThreadPoolExecutor**
- Be testable and CLI-configurable

## Features

- âœ… Efficient chunked reading to avoid memory overload  
- âœ… Data cleaning: remove negatives, standardize case, timestamp enrichment  
- âœ… Temp file output for each chunk (stored in `/temp_chunks`)  
- âœ… Final output merge into a single `.csv` file  
- âœ… Batch inserts into SQLite  
- âœ… Detailed logging of errors  
- âœ… CLI interface for configuration  
- âœ… Parallel processing using `concurrent.futures`  
- âœ… Unit testing support

## Project Structure

```
etl_project/
â”œâ”€â”€ main.py                      # Entry point for the pipeline
â”œâ”€â”€ config.py                    # Default config (overridden via CLI)
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ file_reader.py           # Reads CSV in chunks
â”‚   â”œâ”€â”€ transformer.py           # Cleans and transforms data
â”‚   â”œâ”€â”€ db_writer.py             # DB connection and insertions
â”‚   â””â”€â”€ utils.py                 # Logger setup and helpers
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_transformer.py      # Tests for transformation logic
â”‚   â””â”€â”€ test_db_writer.py        # Tests for DB insertion
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ error.log                # Logs errors during execution
â”œâ”€â”€ temp_chunks/                # Temporary chunk storage
â”œâ”€â”€ sample.csv               # Sample CSV (for testing)
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # Project documentation
```

## Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/guptatejas958/ETL_CSV.git
cd ETL_CSV
```

### 2. Set Up Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate      
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

## ðŸš€ How to Run

You can run the pipeline using the default values in `config.py`, or pass arguments via CLI.

### Option 1: Default config

```bash
python main.py
```

### Option 2: CLI Arguments

```bash
python main.py --csv_path "test_file.csv" --db_path "output.db" --table_name "cleaned_data" --chunk_size 1000 --temp_dir "chunks" --final_csv "cleaned_output.csv" --max_workers 10
```

## Module Responsibilities

### `etl/file_reader.py`
- Uses `pandas.read_csv(..., chunksize=n)`
- Generator yielding one chunk at a time

### `etl/transformer.py`
- Lowercases the `status` field
- Removes rows with:
  - Negative `amount`
  - `status == "cancelled"`
- Adds `processed_at` (UTC)

### `etl/db_writer.py`
- Creates SQLite connection
- Creates target table if missing
- Uses `executemany()` for batch inserts

### `etl/utils.py`
- Sets up a rotating log to `logs/error.log`
- Timestamps and formats logs

### `main.py`
- Orchestrates entire pipeline:
  - Read â†’ Transform â†’ Save chunk â†’ Merge â†’ Load DB
  - Executes transformations in parallel

## Testing

Iâ€™ve implemented two primary test modules using `pytest`.
End-to-End integration testing has also been done with 5GB test_file with logs as below. (go to link)
https://drive.google.com/file/d/1ajyroMsUK7h2i8ccYjycDivVMC__f8MA/view?usp=sharing

### 1. `test_transformer.py`

```python
def test_transform_chunk_removes_cancelled_and_negative():
    df = pd.DataFrame({...})
    cleaned = transform_chunk(df)
    assert ...  # assert cancelled/negative rows removed
```

### 2. `test_db_writer.py`

```python
def test_insert_dataframe():
    conn = create_connection(":memory:")
    df = pd.DataFrame({...})
    insert_dataframe(conn, df, "transactions")
    ...
```

### Run Tests

```bash
pytest tests/
```

## Development Timeline

| Stage                     | Time Spent |
|---------------------------|------------|
| Initial Development       | 3 hours    |
| Debugging & CLI Integration | 2 hours |
| Stress Testing (5GB CSV)  | 2.5 hours  |

## Example Log Output

```
[INFO] Starting ETL pipeline with parallel processing...
[INFO] Processed and saved chunk 0
[INFO] Processed and saved chunk 1
[INFO] Merged final CSV written to cleaned_output.csv
[INFO] Inserted 5000 rows into database.
[INFO] ETL pipeline completed successfully.
```

In case of faulty data:

```
[ERROR] Error processing chunk 12: ValueError: could not convert string to float: 'abc'
```

## Author

Built by [Tejas Gupta]  
guptatejas958@gmail.com
